{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crWGu4dEp0u7"
      },
      "outputs": [],
      "source": [
        "!pip3 install torch torchvision datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXB1WA36Eucy"
      },
      "outputs": [],
      "source": [
        "#Import the necessary libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import copy\n",
        "from datasets import load_dataset\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYuY4X7Yxqbq"
      },
      "outputs": [],
      "source": [
        "#Load a pretrained ResNet model and remove the final classification layer\n",
        "resnet18 = models.resnet18(pretrained = True)\n",
        "\n",
        "model = torch.nn.Sequential(*list(resnet18.children())[:-1]) # Remove the final fully connected layer\n",
        "\n",
        "model.eval()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7J5uOo6nxVu"
      },
      "source": [
        "# CIFAR-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZl3Az4On0JA"
      },
      "outputs": [],
      "source": [
        "# Define transformations for the CIFAR-10 dataset and resize for ResNet\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize CIFAR-10 images to 224x224 for ResNet input\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])  #Normalize as ResNet expects\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CxSngbVoJal"
      },
      "outputs": [],
      "source": [
        "#Load the CIFAR-10 dataset with transformations\n",
        "trainset = torchvision.datasets.CIFAR10(root = './data', train = True, download = True, transform = transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train = False, download = True, transform = transform)\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size = 64, shuffle = False, num_workers = 4, pin_memory = True)\n",
        "testloader = DataLoader(testset, batch_size = 64, shuffle = False, num_workers = 4, pin_memory = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bdi4uyIjqSRO"
      },
      "outputs": [],
      "source": [
        "# Function to extract embeddings for a given dataset\n",
        "def extract_embeddings(dataloader, model, device):\n",
        "    embeddings = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, lbls in dataloader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images).squeeze()  # Remove any extra dimensions, outputs will be (batch_size, 512)\n",
        "            embeddings.append(outputs.cpu())  # Store the embeddings as tensors on CPU\n",
        "            labels.append(lbls)\n",
        "\n",
        "    embeddings = torch.cat(embeddings, dim = 0)  # Concatenate all batch embeddings into one tensor\n",
        "    labels = torch.cat(labels, dim = 0)  # Concatenate all batch labels\n",
        "    return embeddings, labels\n",
        "\n",
        "# Extract embeddings for train and test set\n",
        "train_embeddings, train_labels = extract_embeddings(trainloader, model, device)\n",
        "test_embeddings, test_labels = extract_embeddings(testloader, model, device)\n",
        "\n",
        "# Save the embeddings for future use (optional)\n",
        "torch.save(train_embeddings, 'train_embeddings.pth')\n",
        "torch.save(test_embeddings, 'test_embeddings.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtQe6NgvxcdY"
      },
      "source": [
        "# CIFAR-100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXry7kUQxe_v"
      },
      "outputs": [],
      "source": [
        "# Define transformations for the CIFAR-100 dataset and resize for ResNet\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize CIFAR-100 images to 224x224 for ResNet input\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])  #Normalize as ResNet expects\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWh3xxpmxikR"
      },
      "outputs": [],
      "source": [
        "#Load the CIFAR-100 dataset with transformations\n",
        "trainset = torchvision.datasets.CIFAR100(root = './data', train = True, download = True, transform = transform)\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train = False, download = True, transform = transform)\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size = 64, shuffle = False, num_workers = 4, pin_memory = True)\n",
        "testloader = DataLoader(testset, batch_size = 64, shuffle = False, num_workers = 4, pin_memory = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NNCnf8wxmht"
      },
      "outputs": [],
      "source": [
        "# Function to extract embeddings for a given dataset\n",
        "def extract_embeddings(dataloader, model, device):\n",
        "    embeddings = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, lbls in dataloader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images).squeeze()  # Remove any extra dimensions, outputs will be (batch_size, 512)\n",
        "            embeddings.append(outputs.cpu())  # Store the embeddings as tensors on CPU\n",
        "            labels.append(lbls)\n",
        "\n",
        "    embeddings = torch.cat(embeddings, dim = 0)  # Concatenate all batch embeddings into one tensor\n",
        "    labels = torch.cat(labels, dim = 0)  # Concatenate all batch labels\n",
        "    return embeddings, labels\n",
        "\n",
        "# Extract embeddings for train and test set\n",
        "train_embeddings, train_labels = extract_embeddings(trainloader, model, device)\n",
        "test_embeddings, test_labels = extract_embeddings(testloader, model, device)\n",
        "\n",
        "# Save the embeddings for future use (optional)\n",
        "torch.save(train_embeddings, 'train_embeddings.pth')\n",
        "torch.save(test_embeddings, 'test_embeddings.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYHtuTavyD33"
      },
      "source": [
        "# ImageNet-1K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5L3eiXwyHuv"
      },
      "outputs": [],
      "source": [
        "# Define transformations to apply to images (resize to 256x256 and convert to tensor)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0izzx3nE17s"
      },
      "outputs": [],
      "source": [
        "# Load the ImageNet 1K resized dataset\n",
        "dataset = load_dataset(\"evanarlian/imagenet_1k_resized_256\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xJv83cREyU7"
      },
      "outputs": [],
      "source": [
        "# Custom Dataset class to apply transforms on the dataset\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, hf_dataset, transform=None):\n",
        "        self.dataset = hf_dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # The 'image' is already a PIL image, no need to use Image.open()\n",
        "        image = self.dataset[idx]['image']\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = self.dataset[idx]['label']  # Get the label\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asWF7nDJEs3C",
        "outputId": "a23d5452-bc83-42de-e47b-7828479ad382"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "# Create PyTorch datasets for train and validation sets\n",
        "train_dataset = CustomImageDataset(dataset['train'], transform = transform)\n",
        "test_dataset = CustomImageDataset(dataset['val'], transform = transform)\n",
        "\n",
        "# Create DataLoaders for batch processing\n",
        "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = False, num_workers = 4)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 32, shuffle = False, num_workers = 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMpz8l_PGUdw"
      },
      "outputs": [],
      "source": [
        "# Function to get embeddings\n",
        "def get_embeddings_and_labels(dataloader, model, device):\n",
        "    embeddings = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, label_batch in dataloader:  # Retrieve both images and labels\n",
        "            images = images.to(device)\n",
        "            output = model(images)  # Get embeddings\n",
        "            output = output.view(output.size(0), -1)  # Flatten to (batch_size, 512)\n",
        "\n",
        "            embeddings.append(output.cpu())  # Move embeddings to CPU and store\n",
        "            labels.append(label_batch.cpu())  # Move labels to CPU and store\n",
        "\n",
        "    # Concatenate all embeddings and labels into single tensors\n",
        "    embeddings = torch.cat(embeddings, dim=0)\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "\n",
        "    return embeddings, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTmQm9aq01bB"
      },
      "outputs": [],
      "source": [
        "# Get embeddings and labels for the training set\n",
        "train_embeddings, train_labels = get_embeddings_and_labels(train_loader, model, device)\n",
        "\n",
        "# Get embeddings and labels for the validation (test) set\n",
        "test_embeddings, test_labels = get_embeddings_and_labels(test_loader, model, device)\n",
        "\n",
        "# Save training embeddings and labels\n",
        "torch.save({'embeddings': train_embeddings, 'labels': train_labels}, \"imagenet_resnet18_train_embeddings_and_labels.pt\")\n",
        "\n",
        "# Save validation (test) embeddings and labels\n",
        "torch.save({'embeddings': test_embeddings, 'labels': test_labels}, \"imagenet_resnet18_test_embeddings_and_labels.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQqNRMVX0CUX"
      },
      "source": [
        "# K-Means Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4N-skNwm1COa"
      },
      "outputs": [],
      "source": [
        "# Convert the list of tensors to a 2D NumPy array\n",
        "train_embed = np.array(train_embeddings)\n",
        "\n",
        "# Specify the number of clusters\n",
        "n_clusters = 10\n",
        "\n",
        "# Create a KMeans instance and fit the model\n",
        "kmeans = KMeans(n_clusters = n_clusters, random_state = 42)\n",
        "kmeans.fit(train_embed)\n",
        "\n",
        "# Get the labels (cluster assignments) for each tensor\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Optionally, you can save the labels if needed\n",
        "torch.save(labels, 'cluster_labels.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2shcTNLcI92H"
      },
      "outputs": [],
      "source": [
        "test_embed = np.array(test_embeddings)\n",
        "\n",
        "predicted_labels = kmeans.predict(test_embed) #Predict the clusters for test embeddings\n",
        "\n",
        "torch.save(predicted_labels, 'test_cluster_labels.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1u66CSVCckdN"
      },
      "outputs": [],
      "source": [
        "#Function to compute Precision@k\n",
        "def precision_at_k(query_label, output_docs, k):\n",
        "    # Get the top-k documents\n",
        "    top_k_docs = output_docs[:k]\n",
        "\n",
        "    # Count relevant documents (matching query label)\n",
        "    relevant_count = (top_k_docs == query_label).sum().item()\n",
        "\n",
        "    # Calculate precision\n",
        "    precision = relevant_count / k\n",
        "    return precision\n",
        "\n",
        "def mean_average_precision(retrieved_labels, true_label):\n",
        "    relevant_count = 0\n",
        "    precision_sum = 0.0\n",
        "    for i, label in enumerate(retrieved_labels):\n",
        "        if label == true_label:\n",
        "            relevant_count += 1\n",
        "            precision_sum += relevant_count / (i + 1)\n",
        "    return precision_sum / relevant_count if relevant_count > 0 else 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84OrVGOkLngS"
      },
      "outputs": [],
      "source": [
        "avg_precisions, p_10, p_50 = [], [], []\n",
        "\n",
        "for i, test_tensor in enumerate(test_embed):\n",
        "    cluster_label = predicted_labels[i]  # Get the cluster label for the current test tensor\n",
        "\n",
        "    # Get train tensors that belong to the same cluster\n",
        "    cluster_train_indices = np.where(labels == cluster_label)[0]\n",
        "    cluster_train_data = train_embed[cluster_train_indices]\n",
        "\n",
        "    # Compute cosine similarity between the test tensor and the train tensors in the same cluster\n",
        "    similarities = cosine_similarity([test_tensor], cluster_train_data)[0]\n",
        "\n",
        "    # Rank train tensors by similarity\n",
        "    ranked_indices = np.argsort(similarities)[::-1]  # Sort indices in descending order\n",
        "    ranked_train_indices = cluster_train_indices[ranked_indices]\n",
        "\n",
        "    true_train_labels = train_labels[ranked_train_indices]\n",
        "    true_test_label = test_labels[i]\n",
        "\n",
        "    precision_at_10 = precision_at_k(true_test_label, true_train_labels, 10)\n",
        "    precision_at_50 = precision_at_k(true_test_label, true_train_labels, 50)\n",
        "\n",
        "    p_10.append(precision_at_10)\n",
        "    p_50.append(precision_at_50)\n",
        "\n",
        "    # relevant_indices = np.where(true_train_labels == true_test_label)[0]\n",
        "    # precision_scores = [precision_at_k(true_test_label, true_train_labels, j+1) for j in relevant_indices]\n",
        "    map = mean_average_precision(true_test_label, true_train_labels)\n",
        "\n",
        "    if len(map) == 0:\n",
        "      average_precision = 0\n",
        "\n",
        "    # else:\n",
        "    #   average_precision = sum(precision_scores) / len(precision_scores)\n",
        "\n",
        "    avg_precisions.append(map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoP7lHeEkgSz"
      },
      "outputs": [],
      "source": [
        "mean_p10 = sum(p_10) / len(p_10)\n",
        "mean_p50 = sum(p_50) / len(p_50)\n",
        "mean_ap = sum(avg_precisions) / len(avg_precisions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ah9_erS5n6Bm",
        "outputId": "e611ed5a-af0b-49a1-cffe-1d90a25aba07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7511599999999902\n",
            "0.7077839999999819\n",
            "0.6093555005947813\n"
          ]
        }
      ],
      "source": [
        "print('Mean Precision@10:', mean_p10)\n",
        "print('Mean Precision@50:', mean_p50)\n",
        "print('Mean Average Precision', mean_ap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoBnW5n1zanw"
      },
      "source": [
        "# K-Means Clustering + PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOjFLM4ozf8L"
      },
      "outputs": [],
      "source": [
        "# Create a PCA object\n",
        "pca = PCA(n_components = 0.95)  # Retain 95% of variance\n",
        "\n",
        "train_pca = pca.fit_transform(train_embeddings)\n",
        "test_pca = pca.transform(test_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Se6cNZu0Egx"
      },
      "outputs": [],
      "source": [
        "train_pca = torch.from_numpy(train_pca)\n",
        "test_pca = torch.from_numpy(test_pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTLYh7se0FPp"
      },
      "outputs": [],
      "source": [
        "# Convert the list of tensors to a 2D NumPy array\n",
        "train_embed = np.array(train_pca)\n",
        "\n",
        "# Specify the number of clusters\n",
        "n_clusters = 10\n",
        "\n",
        "# Create a KMeans instance and fit the model\n",
        "kmeans = KMeans(n_clusters = n_clusters, random_state = 42)\n",
        "kmeans.fit(train_embed)\n",
        "\n",
        "# Get the labels (cluster assignments) for each tensor\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Optionally, you can save the labels if needed\n",
        "torch.save(labels, 'cluster_labels.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxwEOlRU0QeI"
      },
      "outputs": [],
      "source": [
        "test_embed = np.array(test_embeddings)\n",
        "\n",
        "predicted_labels = kmeans.predict(test_embed) #Predict the clusters for test embeddings\n",
        "\n",
        "torch.save(predicted_labels, 'test_cluster_labels.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQQ6m08W0SzT"
      },
      "outputs": [],
      "source": [
        "#Function to compute Precision@k\n",
        "def precision_at_k(query_label, output_docs, k):\n",
        "    # Get the top-k documents\n",
        "    top_k_docs = output_docs[:k]\n",
        "\n",
        "    # Count relevant documents (matching query label)\n",
        "    relevant_count = (top_k_docs == query_label).sum().item()\n",
        "\n",
        "    # Calculate precision\n",
        "    precision = relevant_count / k\n",
        "    return precision\n",
        "\n",
        "def mean_average_precision(retrieved_labels, true_label):\n",
        "    relevant_count = 0\n",
        "    precision_sum = 0.0\n",
        "    for i, label in enumerate(retrieved_labels):\n",
        "        if label == true_label:\n",
        "            relevant_count += 1\n",
        "            precision_sum += relevant_count / (i + 1)\n",
        "    return precision_sum / relevant_count if relevant_count > 0 else 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4PLRy550UzB"
      },
      "outputs": [],
      "source": [
        "avg_precisions, p_10, p_50 = [], [], []\n",
        "\n",
        "for i, test_tensor in enumerate(test_embed):\n",
        "    cluster_label = predicted_labels[i]  # Get the cluster label for the current test tensor\n",
        "\n",
        "    # Get train tensors that belong to the same cluster\n",
        "    cluster_train_indices = np.where(labels == cluster_label)[0]\n",
        "    cluster_train_data = train_embed[cluster_train_indices]\n",
        "\n",
        "    # Compute cosine similarity between the test tensor and the train tensors in the same cluster\n",
        "    similarities = cosine_similarity([test_tensor], cluster_train_data)[0]\n",
        "\n",
        "    # Rank train tensors by similarity\n",
        "    ranked_indices = np.argsort(similarities)[::-1]  # Sort indices in descending order\n",
        "    ranked_train_indices = cluster_train_indices[ranked_indices]\n",
        "\n",
        "    true_train_labels = train_labels[ranked_train_indices]\n",
        "    true_test_label = test_labels[i]\n",
        "\n",
        "    precision_at_10 = precision_at_k(true_test_label, true_train_labels, 10)\n",
        "    precision_at_50 = precision_at_k(true_test_label, true_train_labels, 50)\n",
        "\n",
        "    p_10.append(precision_at_10)\n",
        "    p_50.append(precision_at_50)\n",
        "\n",
        "    # relevant_indices = np.where(true_train_labels == true_test_label)[0]\n",
        "    # precision_scores = [precision_at_k(true_test_label, true_train_labels, j+1) for j in relevant_indices]\n",
        "\n",
        "    map = mean_average_precision(true_test_label, true_train_labels)\n",
        "\n",
        "    if len(map) == 0:\n",
        "      average_precision = 0\n",
        "\n",
        "    # else:\n",
        "    #   average_precision = sum(precision_scores) / len(precision_scores)\n",
        "\n",
        "    avg_precisions.append(map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tarptyb30Wf6"
      },
      "outputs": [],
      "source": [
        "mean_p10 = sum(p_10) / len(p_10)\n",
        "mean_p50 = sum(p_50) / len(p_50)\n",
        "mean_ap = sum(avg_precisions) / len(avg_precisions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trBhuRLM0YnM"
      },
      "outputs": [],
      "source": [
        "print('Mean Precision@10:', mean_p10)\n",
        "print('Mean Precision@50:', mean_p50)\n",
        "print('Mean Average Precision', mean_ap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uPMkoh4jN5V"
      },
      "source": [
        "# Random Hyperplanes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4NkvPsyt7im"
      },
      "outputs": [],
      "source": [
        "train_embed = np.array(train_embeddings)\n",
        "test_embed = np.array(test_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekEZG8NKjQWV"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "hyperplanes = 10 #This value is changed with [3, 4, 7, 8, 10, 11]\n",
        "hashtables = 12 #This value is changed with [3, 6, 12]\n",
        "\n",
        "plane_norms = torch.rand(hashtables, hyperplanes, 512) - 0.5\n",
        "\n",
        "product = torch.matmul(train_embeddings, plane_norms.transpose(-1, -2))\n",
        "product = product > 0\n",
        "product = product.long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGWvsjqNydEZ"
      },
      "outputs": [],
      "source": [
        "bucket_list = []\n",
        "\n",
        "binary_dict = {}\n",
        "\n",
        "for a in range(2 ** hyperplanes):\n",
        "    binary_number = format(a, '0' + str(hyperplanes) + 'b')\n",
        "    binary_dict[binary_number] = []\n",
        "\n",
        "for i in range(hashtables):\n",
        "  buckets = copy.deepcopy(binary_dict)\n",
        "\n",
        "  for j in range(len(product[i])):\n",
        "    hash_str = ''.join(map(str, product[i][j].tolist()))\n",
        "\n",
        "    buckets[hash_str].append(j)\n",
        "\n",
        "  bucket_list.append(buckets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSWFE09yjmEc"
      },
      "outputs": [],
      "source": [
        "test_product = torch.matmul(test_embeddings, plane_norms.transpose(-1, -2))\n",
        "test_product = test_product > 0\n",
        "test_product = test_product.long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIBMUygUsfhG"
      },
      "outputs": [],
      "source": [
        "# Compute cosine similarity and rank train tensors for each test tensor\n",
        "avg_precisions, p_10, p_50 = [], [], []\n",
        "\n",
        "for i, test_tensor in enumerate(test_embed):\n",
        "  cluster_train_indices = []\n",
        "\n",
        "  for j in range(hashtables):\n",
        "    bucket_code = ''.join(map(str, test_product[j][i].tolist()))\n",
        "\n",
        "    cluster_train_indices.append(bucket_list[j][bucket_code])\n",
        "\n",
        "  cluster_train_indices = [a for b in cluster_train_indices for a in b]\n",
        "  cluster_train_indices = np.array(list(set(cluster_train_indices)))\n",
        "\n",
        "  cluster_train_data = train_embed[cluster_train_indices]\n",
        "\n",
        "  # Compute cosine similarity between the test tensor and the train tensors in the same cluster\n",
        "  similarities = cosine_similarity([test_tensor], cluster_train_data)[0]\n",
        "\n",
        "  # Rank train tensors by similarity\n",
        "  ranked_indices = np.argsort(similarities)[::-1]  # Sort indices in descending order\n",
        "  ranked_train_indices = cluster_train_indices[ranked_indices]\n",
        "\n",
        "  true_train_labels = train_labels[ranked_train_indices]\n",
        "  true_test_label = test_labels[i]\n",
        "\n",
        "  precision_at_10 = precision_at_k(true_test_label, true_train_labels, 10)\n",
        "  precision_at_50 = precision_at_k(true_test_label, true_train_labels, 50)\n",
        "\n",
        "  p_10.append(precision_at_10)\n",
        "  p_50.append(precision_at_50)\n",
        "\n",
        "  # relevant_indices = np.where(true_train_labels == true_test_label)[0]\n",
        "  # precision_scores = [precision_at_k(true_test_label, true_train_labels, j+1) for j in relevant_indices]\n",
        "\n",
        "  map = mean_average_precision(true_test_label, true_train_labels)\n",
        "\n",
        "  if len(map) == 0:\n",
        "    average_precision = 0\n",
        "\n",
        "    # else:\n",
        "    #   average_precision = sum(precision_scores) / len(precision_scores)\n",
        "\n",
        "  avg_precisions.append(map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjT_8JzqzCiF"
      },
      "outputs": [],
      "source": [
        "mean_p10 = sum(p_10) / len(p_10)\n",
        "mean_p50 = sum(p_50) / len(p_50)\n",
        "mean_ap = sum(avg_precisions) / len(avg_precisions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg6ulrWVzFi-",
        "outputId": "e0d957fd-282a-4703-fa8b-8f74a59457af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7624499999999887\n",
            "0.7081439999999855\n",
            "0.4274341529613439\n"
          ]
        }
      ],
      "source": [
        "print('Mean Precision@10: ', mean_p10)\n",
        "print('Mean Precision@50: ', mean_p50)\n",
        "print('Mean Average Precision: ', mean_ap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural LSH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('cifar10_train_feature.pkl', 'wb') as f:  \n",
        "    pickle.dump(train_embeddings, f)\n",
        "\n",
        "with open('cifar10_test_feature.pkl', 'wb') as f:  \n",
        "    pickle.dump(test_embeddings, f)\n",
        "    \n",
        "with open('cifar10_train_feature.pkl',mode='rb') as f:\n",
        "    train_embeddings = pickle.load(f)\n",
        "    \n",
        "with open('cifar10_test_feature.pkl',mode='rb') as f:\n",
        "    test_embeddings = pickle.load(f)\n",
        "\n",
        "train_labels1_set = train_labels.reshape(50000,1)\n",
        "train_set = torch.cat((train_embeddings, train_labels1_set), dim=1)\n",
        "\n",
        "test_labels1_set = test_labels.reshape(10000,1)\n",
        "test_set = torch.cat((test_embeddings, test_labels1_set), dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch \n",
        "\n",
        "# Assuming 'train' is a numpy array where the last column contains the class labels\n",
        "labels = train_set[:, -1]\n",
        "\n",
        "# Create a binary mask by comparing each pair of labels\n",
        "masking = (labels[:, None] != labels).to(torch.int)\n",
        "masking = torch.triu(mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Define the compression network C_psi (same as before)\n",
        "class CompNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(CompNetwork, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # The output needs to be in [-1, 1], so we use tanh activation\n",
        "        return torch.tanh(self.linear(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Objective function terms (as before)\n",
        "def bit_balance_loss(p):\n",
        "    # print(torch.sum(b, dim=1).shape)\n",
        "    # print(torch.abs(torch.sum(b, dim=1)).shape)\n",
        "    # print(torch.mean(torch.abs(torch.sum(b, dim=1))))\n",
        "    \n",
        "    return torch.mean(torch.abs(torch.sum(p, dim=1)))\n",
        "\n",
        "def sitting_on_the_fence_loss(p):\n",
        "    # print(torch.abs(torch.abs(b) - 1).shape)\n",
        "    # print(torch.mean(torch.abs((torch.abs(b) - 1))))\n",
        "    return torch.mean(torch.abs((torch.abs(p) - 1)))\n",
        "\n",
        "def weak_supervision_loss(p, masking):\n",
        "    # Weak supervision loss with negative sampling\n",
        "    # Encourage dissimilar hash codes for negative pairs (image pairs)\n",
        "    loss = 0\n",
        "    cr = torch.mm(p, p.T)\n",
        "    result_matrix = cr * masking\n",
        "    abs_result_matrix = torch.abs(result_matrix)\n",
        "    loss = torch.sum(abs_result_matrix)/1125000000\n",
        "    # print(loss)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hashing process based on the additional details (unchanged)\n",
        "def create_hash_buckets(p_u, L, J):\n",
        "    N, H = p_u.shape\n",
        "    buckets = []\n",
        "    \n",
        "    for _ in range(L):\n",
        "        i = np.random.choice(H, J, replace=False)\n",
        "        bucket = p_u[:, i]\n",
        "        buckets.append(bucket)\n",
        "    \n",
        "    return buckets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters (as before)\n",
        "alpha = 0.3\n",
        "beta = 0.3\n",
        "gamma = 0.4\n",
        "L = 10\n",
        "J = 16\n",
        "input_dimension = 2048\n",
        "output_dimension = 500\n",
        "\n",
        "# Instantiate the network\n",
        "model = CompNetwork(input_dimension, output_dimension)\n",
        "\n",
        "# Dummy data for images: Random features for N images, D-dimensional\n",
        "# N = 100  # Number of images\n",
        "D = input_dimension\n",
        "X = train_set[:, :-1]  # Random embeddings for images\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "for epoch in range(20):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass through the network\n",
        "    p_u = model(X)\n",
        "\n",
        "    # Compute the surrogate binary hash code\n",
        "    # binary_hash = torch.sign(b_u)\n",
        "\n",
        "    # Sample negative pairs (for weak supervision)\n",
        "    # negative_pairs = sample_negatives(X, batch_size=10, num_negatives=5)\n",
        "\n",
        "    # Compute the losses\n",
        "    balance = bit_balance_loss(p_u)\n",
        "    fence = sitting_on_the_fence_loss(p_u)\n",
        "    weak_supervision = weak_supervision_loss(p_u, masking)\n",
        "    \n",
        "    # Total loss as per the objective function\n",
        "    loss = (alpha * balance) + (beta * fence) + (gamma * supervision)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # print(f'Epoch {epoch}, Loss: {loss.item()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate the hash buckets (unchanged)\n",
        "model.eval()\n",
        "p_u = model(X)\n",
        "binary_hash = torch.sign(p_u)\n",
        "hash_buckets = create_hash_buckets(binary_hash.detach().numpy(), L, J)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "k = copy.deepcopy(hash_buckets)\n",
        "k[k == -1] = 0 #replacing -1 in the hash bit with 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#creating dictionary with key as hascode and value as image indices\n",
        "# Dictionary to hold the result\n",
        "binary_dict = {}\n",
        "\n",
        "# Loop through the 10 numpy arrays\n",
        "for i, array_50000 in enumerate(k):\n",
        "    \n",
        "    # Loop through each of the 50000 arrays in the current numpy array\n",
        "    for j, binary_array in enumerate(array_50000):\n",
        "        \n",
        "        # Convert the binary numpy array to a tuple (so it can be used as a dictionary key)\n",
        "        binary_tuple = tuple(binary_array)\n",
        "        \n",
        "        # If the binary vector is not in the dictionary, initialize with an empty list\n",
        "        if binary_tuple not in binary_dict:\n",
        "            binary_dict[binary_tuple] = []\n",
        "        \n",
        "        # Append the current index (j) to the value list\n",
        "        binary_dict[binary_tuple].append(j)\n",
        "\n",
        "# Now `binary_dict` contains keys as 16-dimensional binary arrays (tuples)\n",
        "# and values as the indices of the corresponding samples\n",
        "print(binary_dict)  # Number of unique 16-dimensional binary arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#loading the trained model\n",
        "model_path = \"CIFAR10_500.pth\"\n",
        "models.load_state_dict(torch.load(model_path))\n",
        "models.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cosine similarity between test image and retrieved images\n",
        "def compute_cosine_similarity(test_embedding, retrieved_embeddings):\n",
        "    similarities = cosine_similarity(test_embedding.reshape(1, -1), retrieved_embeddings)\n",
        "    return similarities.squeeze()\n",
        "\n",
        "# Precision@K\n",
        "def precision_at_k(retrieved_labels, true_label, k):\n",
        "    relevant = sum([1 for label in retrieved_labels[:k] if label == true_label])\n",
        "    return relevant / k\n",
        "\n",
        "# Mean Average Precision (MAP)\n",
        "def mean_average_precision(retrieved_labels, true_label):\n",
        "    relevant_count = 0\n",
        "    precision_sum = 0.0\n",
        "    for i, label in enumerate(retrieved_labels):\n",
        "        if label == true_label:\n",
        "            relevant_count += 1\n",
        "            precision_sum += relevant_count / (i + 1)\n",
        "    return precision_sum / relevant_count if relevant_count > 0 else 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find images in the same bucket for test image and evaluate\n",
        "def evaluate_lsh(test_embeddings, test_labels, train_embeddings, train_labels,num_tables, binary_dict):\n",
        "    precisions_10 = []\n",
        "    precisions_50 = []\n",
        "    mean_avg_precisions = []\n",
        "    \n",
        "#     Xt = test[:, :-1]\n",
        "    Xt = test_embeddings\n",
        "    b_ut = models(Xt)\n",
        "    binary_hash_test = torch.sign(b_ut)\n",
        "\n",
        "    for i, test_hash in enumerate(binary_hash_test):\n",
        "        test_label = test_labels[i]\n",
        "\n",
        "        # Retrieve images from the corresponding bucket in each hash table\n",
        "        retrieved_indices = set()\n",
        "        for l in range(num_tables):\n",
        "            test_hash_code = test_hash\n",
        "#             print('test_hash_code: ', test_hash_code)\n",
        "#             bucket = binary_dict[l].get(test_hash_code, [])\n",
        "            bucket = binary_dict[test_hash_code]\n",
        "            retrieved_indices.update(bucket)\n",
        "\n",
        "        retrieved_indices = list(retrieved_indices)\n",
        "        if len(retrieved_indices) == 0:\n",
        "            continue\n",
        "\n",
        "        retrieved_embeddings = train_embeddings[retrieved_indices]\n",
        "        retrieved_labels = train_labels[retrieved_indices]\n",
        "        # retrieved_embeddings = binary_dict[retrieved_indices]\n",
        "        # retrieved_labels = binary_dict[retrieved_indices]\n",
        "\n",
        "        # Compute cosine similarity\n",
        "        similarities = compute_cosine_similarity(test_embeddings, retrieved_embeddings)\n",
        "\n",
        "        # Rank the retrieved images by similarity\n",
        "        ranked_indices = np.argsort(-similarities)  # Sort in descending order\n",
        "        ranked_labels = [retrieved_labels[idx] for idx in ranked_indices]\n",
        "\n",
        "        # Compute Precision@10, Precision@50, and MAP\n",
        "        precision_10 = precision_at_k(ranked_labels, test_label, 10)\n",
        "        precision_50 = precision_at_k(ranked_labels, test_label, 50)\n",
        "        map_score = mean_average_precision(ranked_labels, test_label)\n",
        "\n",
        "        precisions_10.append(precision_10)\n",
        "        precisions_50.append(precision_50)\n",
        "        mean_avg_precisions.append(map_score)\n",
        "\n",
        "        # Print results for this test image (optional)\n",
        "#         print(f\"Test Image {i+1}: Precision@10: {precision_10:.4f}, Precision@50: {precision_50:.4f}, MAP: {map_score:.4f}\")\n",
        "    \n",
        "    # Average precision across all test images\n",
        "    avg_precision_10 = np.mean(precisions_10)\n",
        "    avg_precision_50 = np.mean(precisions_50)\n",
        "    avg_map = np.mean(mean_avg_precisions)\n",
        "\n",
        "    print(f\"\\nOverall Results - Precision@10: {avg_precision_10:.4f}, Precision@50: {avg_precision_50:.4f}, MAP: {avg_map:.4f}\")\n",
        "    \n",
        "    return avg_precision_10, avg_precision_50, avg_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate LSH on the test set\n",
        "avg_precision_10, avg_precision_50, avg_map = evaluate_lsh(\n",
        "    test_embeddings=test_set[:,:-1],\n",
        "    test_labels=test_labels1_set,\n",
        "    train_embeddings = train_set[:, -1],\n",
        "    train_labels = train_labels1_set,\n",
        "    num_tables=L,\n",
        "    binary_dict=binary_dict\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
